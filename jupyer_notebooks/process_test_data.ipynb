{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Number of testing sentences: 14,935\n",
      "\n",
      "Loaded input_ids_test.\n",
      "Max test sentence length:  890\n",
      "\n",
      "Padding/truncating all sentences to 512 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('medicalai/ClinicalBERT', do_lower_case=True)\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_test = pd.read_csv(\"~/med264/Dataset1/day1_30mortality_test.csv\", index_col=0)\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of testing sentences: {:,}\\n'.format(df_test.shape[0]))\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences_test = df_test.TEXT.values\n",
    "labels_test = df_test.Label.values\n",
    "\n",
    "file_path_test = os.path.expanduser('~/med264/Dataset1/input_ids_test.pickle')\n",
    "\n",
    "input_ids_test = []\n",
    "\n",
    "if os.path.exists(file_path_test):\n",
    "    with open(file_path_test, 'rb') as f:\n",
    "        input_ids_test = pickle.load(f)\n",
    "    print('Loaded input_ids_test.')\n",
    "else:\n",
    "    for sent in tqdm(sentences_test):\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n",
    "        input_ids_test.append(encoded_sent)\n",
    "    with open(file_path_test, 'wb') as f:\n",
    "            pickle.dump(input_ids_test, f)\n",
    "    print('Saved input_ids_test.')\n",
    "\n",
    "print('Max test sentence length: ', max([len(sen) for sen in input_ids_test]))\n",
    "\n",
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
    "# maximum training sentence length of 47...\n",
    "MAX_LEN = 512\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\",\n",
    "                            value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')\n",
    "\n",
    "attention_masks_test = []\n",
    "for sent in input_ids_test:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks_test.append(att_mask)\n",
    "\n",
    "test_inputs, test_labels, test_masks = input_ids_test, labels_test, attention_masks_test\n",
    "\n",
    "# Convert all inputs and labels into torch tensors, the required datatype\n",
    "# for our model.\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test_inputs, test_labels, test_masks\n",
    "torch.save(test_inputs, os.path.expanduser('~/med264/Dataset1/test_inputs.pt'))\n",
    "torch.save(test_labels, os.path.expanduser('~/med264/Dataset1/test_labels.pt'))\n",
    "torch.save(test_masks, os.path.expanduser('~/med264/Dataset1/test_masks.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LLM]",
   "language": "python",
   "name": "conda-env-LLM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
