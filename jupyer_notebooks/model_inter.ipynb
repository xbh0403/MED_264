{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX A6000\n",
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"/new-stg/home/banghua/Transformer-Explainability\")\n",
    "\n",
    "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "\n",
    "from captum.attr import visualization\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AdamW, BertConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('medicalai/ClinicalBERT', do_lower_case=True)\n",
    "\n",
    "model_path = os.path.expanduser('~/med264/models_balanced/')\n",
    "preds_path = os.path.expanduser('~/med264/preds_balanced/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing sentences: 3,246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"~/med264/Dataset1/day1_30mortality_test.csv\", index_col=0)\n",
    "num_test_pos = df_test[df_test['Label'] == 1].shape[0]\n",
    "num_test_neg = df_test[df_test['Label'] == 0].shape[0]\n",
    "num_balanced = min(num_test_pos, num_test_neg)\n",
    "df_test_pos = df_test[df_test['Label'] == 1].sample(n=num_balanced, random_state=42)\n",
    "df_test_neg = df_test[df_test['Label'] == 0].sample(n=num_balanced, random_state=42)\n",
    "df_test = pd.concat([df_test_pos, df_test_neg])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of testing sentences: {:,}\\n'.format(df_test.shape[0]))\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences_test = df_test.TEXT.values\n",
    "labels_test = df_test.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input_ids_test.\n",
      "Max test sentence length:  878\n"
     ]
    }
   ],
   "source": [
    "# Correct the path by expanding the tilde to the user's home directory\n",
    "file_path_test = os.path.expanduser('~/med264/Dataset2/input_ids_test.pickle')\n",
    "\n",
    "\n",
    "# input_ids_train, input_ids_valid, input_ids_test = [], [], []\n",
    "input_ids_test = []\n",
    "\n",
    "\n",
    "if os.path.exists(file_path_test):\n",
    "    with open(file_path_test, 'rb') as f:\n",
    "        input_ids_test = pickle.load(f)\n",
    "    print('Loaded input_ids_test.')\n",
    "else:\n",
    "    for sent in tqdm(sentences_test):\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n",
    "        input_ids_test.append(encoded_sent)\n",
    "    with open(file_path_test, 'wb') as f:\n",
    "            pickle.dump(input_ids_test, f)\n",
    "    print('Saved input_ids_test.')\n",
    "    \n",
    "\n",
    "# print('Max train sentence length: ', max([len(sen) for sen in input_ids_train]))\n",
    "# print('Max valid sentence length: ', max([len(sen) for sen in input_ids_valid]))\n",
    "print('Max test sentence length: ', max([len(sen) for sen in input_ids_test]))\n",
    "\n",
    "file_path = os.path.expanduser('~/med264/Dataset2/processed_data.pickle')\n",
    "with open(file_path, 'rb') as f:\n",
    "    input_ids_train, input_ids_valid, input_ids_test, attention_masks_train, attention_masks_valid, attention_masks_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, test_inputs, test_labels =\\\n",
    "input_ids_train, input_ids_valid, input_ids_test, labels_test\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, test_masks = attention_masks_train, attention_masks_valid, attention_masks_test\n",
    "\n",
    "# Convert all inputs and labels into torch tensors, the required datatype\n",
    "# for our model.\n",
    "# train_inputs = torch.tensor(train_inputs)\n",
    "# validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# validation_labels = torch.tensor(validation_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# train_masks = torch.tensor(train_masks)\n",
    "# validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it\n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoader for our test set.\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(preds_path+\"/4/preds.pickle\", \"rb\") as f:\n",
    "    preds_results = pickle.load(f)\n",
    "\n",
    "test_true_labels = preds_results['test_true_labels']\n",
    "test_preds = preds_results['test_preds']\n",
    "\n",
    "# Find test_TPs given test_true_labels and test_preds\n",
    "test_TPs = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 1 and test_preds[i] == 1:\n",
    "        test_TPs.append(i)\n",
    "\n",
    "# Find test_TNs given test_true_labels and test_preds\n",
    "test_TNs = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 0 and test_preds[i] == 0:\n",
    "        test_TNs.append(i)\n",
    "\n",
    "# Find test_FPs given test_true_labels and test_preds\n",
    "test_FPs = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 0 and test_preds[i] == 1:\n",
    "        test_FPs.append(i)\n",
    "\n",
    "# Find test_FNs given test_true_labels and test_preds\n",
    "test_FNs = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 1 and test_preds[i] == 0:\n",
    "        test_FNs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_TPs:  1362 test_TNs:  971 test_FPs:  652 test_FNs:  261\n"
     ]
    }
   ],
   "source": [
    "print(\"test_TPs: \", len(test_TPs), \"test_TNs: \", len(test_TNs), \"test_FPs: \", len(test_FPs), \"test_FNs: \", len(test_FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4:\n",
      "Loading model from /new-stg/home/banghua/med264/models_balanced/4/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /new-stg/home/banghua/med264/models_balanced/4/ and are newly initialized: ['bert.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "\n",
    "print('Model ' + str(i) + ':')\n",
    "model_path_i = os.path.expanduser('~/med264/models_balanced/' + str(i) + '/')\n",
    "print('Loading model from ' + model_path_i)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_path_i, num_labels = 2, output_attentions = False, output_hidden_states = False\n",
    "    )\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "classifications = [\"NOT_DEAD\", \"DEAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(input_ids, attention_mask, true_class, explanations):\n",
    "    # generate an explanation for the input\n",
    "    expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
    "    # normalize scores\n",
    "    expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "    # get the model classification\n",
    "    output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
    "    classification = output.argmax(dim=-1).item()\n",
    "    # get class name\n",
    "    class_name = classifications[classification]\n",
    "    # if the classification is negative, higher explanation scores are more negative\n",
    "    # flip for visualization\n",
    "    if class_name == \"DEAD\":\n",
    "        expl *= (-1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "    tokens_results = [(tokens[i], expl[i].item()) for i in range(len(tokens))]\n",
    "    return tokens_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = Generator(model)\n",
    "\n",
    "def get_many(inputs, masks, labels, explanations):\n",
    "    token_results = []\n",
    "    # viz_data_records = []\n",
    "    \n",
    "    for j in tqdm(range(inputs.shape[0])):\n",
    "        # input_ids, attention_mask, expl, output, classification, tokens, tokens_output = None, None, None, None, None, None, None\n",
    "        input_ids = inputs[j].unsqueeze(0).to(device)\n",
    "        attention_mask = masks[j].unsqueeze(0).to(device)\n",
    "        input_label = labels[j]\n",
    "        # tokens_output, viz = process_one(input_ids, attention_mask, input_label, explanations)\n",
    "        tokens_output = process_one(input_ids, attention_mask, input_label, explanations)\n",
    "        token_results.append(tokens_output)\n",
    "    # viz_data_records.append(viz)\n",
    "\n",
    "    return token_results\n",
    "\n",
    "# import pickle\n",
    "# save_file_path = \"/new-stg/home/banghua/med264/trans_inter/token_results_{}.pkl\".format(i)\n",
    "# with open(save_file_path, 'wb') as f:\n",
    "#     pickle.dump(token_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/50 [00:00<?, ?it/s]/new-stg/home/banghua/anaconda3/envs/Trans_Inter/lib/python3.9/site-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:12<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "test_TPs_50_token_results = get_many(test_inputs[test_TPs[:50]], test_masks[test_TPs[:50]], test_labels[test_TPs[:50]], explanations)\n",
    "# Save test_TPs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter/test_TPs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_TPs_50_token_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/50 [00:00<?, ?it/s]/new-stg/home/banghua/anaconda3/envs/Trans_Inter/lib/python3.9/site-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:12<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "test_TNs_50_token_results = get_many(test_inputs[test_TNs[:50]], test_masks[test_TNs[:50]], test_labels[test_TNs[:50]], explanations)\n",
    "# Save test_TNs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter/test_TNs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_TNs_50_token_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/50 [00:00<?, ?it/s]/new-stg/home/banghua/anaconda3/envs/Trans_Inter/lib/python3.9/site-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:12<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "test_FPs_50_token_results = get_many(test_inputs[test_FPs[:50]], test_masks[test_FPs[:50]], test_labels[test_FPs[:50]], explanations)\n",
    "# Save test_FPs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter/test_FPs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_FPs_50_token_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/50 [00:00<?, ?it/s]/new-stg/home/banghua/anaconda3/envs/Trans_Inter/lib/python3.9/site-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:12<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "test_FNs_50_token_results = get_many(test_inputs[test_FNs[:50]], test_masks[test_FNs[:50]], test_labels[test_FNs[:50]], explanations)\n",
    "# Save test_FNs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter/test_FNs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_FNs_50_token_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Trans_Inter]",
   "language": "python",
   "name": "conda-env-Trans_Inter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
