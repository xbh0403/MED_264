{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX A6000\n",
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"/new-stg/home/banghua/Transformer-Explainability\")\n",
    "\n",
    "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "\n",
    "from captum.attr import visualization\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer_path = os.path.expanduser('~/med264/clinicalBERTs/pretraining_checkpoint/')\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n",
    "\n",
    "model_path = os.path.expanduser('~/med264/models_balanced_clinicalBERT/')\n",
    "preds_path = os.path.expanduser('~/med264/preds_balanced_clinicalBERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing sentences: 3,246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"~/med264/Dataset1/day1_30mortality_test.csv\", index_col=0)\n",
    "num_test_pos = df_test[df_test['Label'] == 1].shape[0]\n",
    "num_test_neg = df_test[df_test['Label'] == 0].shape[0]\n",
    "num_balanced = min(num_test_pos, num_test_neg)\n",
    "df_test_pos = df_test[df_test['Label'] == 1].sample(n=num_balanced, random_state=42)\n",
    "df_test_neg = df_test[df_test['Label'] == 0].sample(n=num_balanced, random_state=42)\n",
    "df_test = pd.concat([df_test_pos, df_test_neg])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of testing sentences: {:,}\\n'.format(df_test.shape[0]))\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences_test = df_test.TEXT.values\n",
    "labels_test = df_test.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input_ids_test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 3246/3246 [00:13<00:00, 244.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max test sentence length:  818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Correct the path by expanding the tilde to the user's home directory\n",
    "file_path_test = os.path.expanduser('~/med264/Dataset3/input_ids_test.pickle')\n",
    "\n",
    "\n",
    "# input_ids_train, input_ids_valid, input_ids_test = [], [], []\n",
    "input_ids_test = []\n",
    "input_ids_encode = []\n",
    "\n",
    "if os.path.exists(file_path_test):\n",
    "    with open(file_path_test, 'rb') as f:\n",
    "        input_ids_test = pickle.load(f)\n",
    "    print('Loaded input_ids_test.')\n",
    "# else:\n",
    "    for sent in tqdm(sentences_test):\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n",
    "        input_ids_encode.append(encoded_sent)\n",
    "    # with open(file_path_test, 'wb') as f:\n",
    "    #         pickle.dump(input_ids_test, f)\n",
    "    # print('Saved input_ids_test.')\n",
    "\n",
    "print('Max test sentence length: ', max([len(sen) for sen in input_ids_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Label</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8858</th>\n",
       "      <td>left hemithorax. new hazy increased density ri...</td>\n",
       "      <td>1</td>\n",
       "      <td>106037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42635</th>\n",
       "      <td>there are radiopaque densities at the lung bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>128774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76935</th>\n",
       "      <td>sinus tachycardia. intraventricular conduction...</td>\n",
       "      <td>1</td>\n",
       "      <td>151490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108877</th>\n",
       "      <td>placement of a right frontal approach ventricu...</td>\n",
       "      <td>1</td>\n",
       "      <td>173377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60101</th>\n",
       "      <td>atrial fibrillation with slow ventricular resp...</td>\n",
       "      <td>1</td>\n",
       "      <td>140092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     TEXT  Label      ID\n",
       "8858    left hemithorax. new hazy increased density ri...      1  106037\n",
       "42635   there are radiopaque densities at the lung bas...      1  128774\n",
       "76935   sinus tachycardia. intraventricular conduction...      1  151490\n",
       "108877  placement of a right frontal approach ventricu...      1  173377\n",
       "60101   atrial fibrillation with slow ventricular resp...      1  140092"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_index = df_test.index\n",
    "test_text = df_test.TEXT.values.tolist()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_ids_encode)):\n",
    "    assert input_ids_encode[i] == input_ids_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.expanduser('~/med264/Dataset3/processed_data.pickle')\n",
    "with open(file_path, 'rb') as f:\n",
    "    input_ids_train, input_ids_valid, input_ids_test, attention_masks_train, attention_masks_valid, attention_masks_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'involving the inner and outer table of the left frontal bone, extending to the roof of the left orbit. there are also fractures of the roof of the right orbit. there is a comminuted fracture of the lateral wall of the left orbit. both lamina papyracea are fractured. the right lamina papyracea is fractured posteriorly, with a bone fragment intruding on the apex formed by the extraocular muscles. there is a fracture of the lateral wall of the right orbit posteriorly, with air and blood seen within the middle cranial fossa just posterior to this fracture. there are fractures of the floors of both orbits. the inferior rectus muscles approach the fracture defect, but do not definitely cross through them. the infraorbital foramina are likely involved in the fractures bilaterally. there is a fracture of the roof of the right sphenoid air cell. there are fractures of the lateral and anterior walls of both maxillary sinuses, with an osseous fragment seen within the left maxillary sinus. the maxillary sinuses and ethmoid air cells are filled with blood, as are some portions of the frontal sinus. there are multiple nasal fractures. there is a segmental fracture of the left zygomatic arch. there is soft tissue hematoma centered at the left forehead, with radiodense material noted within the soft tissue anteriorly. there appears to be some mild proptosis of the left globe. the globes appear rounded. there is stranding within the left orbit, predominantly inferior to the globe. a nasogastric tube is noted to be in place. air is seen near the tortuous portion of the carotid arteries bilaterally. impression: extensive facial and cranial fractures as described. the findings are in sum likely keeping with a le type of fracture. findings were discussed with the trauma surgery service as well as the ophthalmology service at the conclusion of the exam. (over) 8:53 pm ct sinus/mandible/maxillofacial w/o contrast clip # reason:'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] involving the inner and outer table of the left frontal bone, extending to the roof of the left orbit. there are also fractures of the roof of the right orbit. there is a comminuted fracture of the lateral wall of the left orbit. both lamina papyracea are fractured. the right lamina papyracea is fractured posteriorly, with a bone fragment intruding on the apex formed by the extraocular muscles. there is a fracture of the lateral wall of the right orbit posteriorly, with air and blood seen within the middle cranial fossa just posterior to this fracture. there are fractures of the floors of both orbits. the inferior rectus muscles approach the fracture defect, but do not definitely cross through them. the infraorbital foramina are likely involved in the fractures bilaterally. there is a fracture of the roof of the right sphenoid air cell. there are fractures of the lateral and anterior walls of both maxillary sinuses, with an osseous fragment seen within the left maxillary sinus. the maxillary sinuses and ethmoid air cells are filled with blood, as are some portions of the frontal sinus. there are multiple nasal fractures. there is a segmental fracture of the left zygomatic arch. there is soft tissue hematoma centered at the left forehead, with radiodense material noted within the soft tissue anteriorly. there appears to be some mild proptosis of the left globe. the globes appear rounded. there is stranding within the left orbit, predominantly inferior to the globe. a nasogastric tube is noted to be in place. air is seen near the tortuous portion of the carotid arteries bilaterally. impression : extensive facial and cranial fractures as described. the findings are in sum likely keeping with a le type of fracture. findings were discussed with the trauma surgery service as well as the ophthalmology service at the conclusion of the exam. ( over ) 8 : 53 pm ct sinus / mandible / maxillofacial w / o contrast clip # reason : [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, test_inputs, test_labels = input_ids_train, input_ids_valid, input_ids_test, labels_test\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, test_masks = attention_masks_train, attention_masks_valid, attention_masks_test\n",
    "\n",
    "# Convert all inputs and labels into torch tensors, the required datatype\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoader for our test set.\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Loading model from /new-stg/home/banghua/med264/models_balanced_clinicalBERT/1/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /new-stg/home/banghua/med264/models_balanced_clinicalBERT/1/ and are newly initialized: ['bert.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "acc_results = {}\n",
    "preds_results = {}\n",
    "\n",
    "i=1\n",
    "print('Model ' + str(i) + ':')\n",
    "model_path_i = model_path + str(i) + '/'\n",
    "print('Loading model from ' + model_path_i + '...')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "model_path_i, num_labels = 2, output_attentions = False, output_hidden_states = False\n",
    ")\n",
    "model.cuda()\n",
    "model.eval()\n",
    "classifications = [\"NOT_DEAD\", \"DEAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_pred(i):\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = test_inputs[i:i+1].cuda(), test_masks[i:i+1].cuda(), test_labels[i:i+1].cuda()\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up prediction\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    pred = np.argmax(logits)\n",
    "    return pred, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/new-stg/home/banghua/anaconda3/envs/Trans_Inter/lib/python3.9/site-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, array([1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_one_pred(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(preds_path+\"/1/preds.pickle\", \"rb\") as f:\n",
    "    preds_results = pickle.load(f)\n",
    "\n",
    "test_true_labels = preds_results['test_true_labels']\n",
    "test_preds = preds_results['test_preds']\n",
    "\n",
    "# Find test_TPs given test_true_labels and test_preds\n",
    "test_TPs = []\n",
    "test_TPs_orig_index = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 1 and test_preds[i] == 1:\n",
    "        test_TPs.append(i)\n",
    "        test_TPs_orig_index.append(orig_index[i])\n",
    "\n",
    "\n",
    "# Find test_TNs given test_true_labels and test_preds\n",
    "test_TNs = []\n",
    "test_TNs_orig_index = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 0 and test_preds[i] == 0:\n",
    "        test_TNs.append(i)\n",
    "        test_TNs_orig_index.append(orig_index[i])\n",
    "\n",
    "# Find test_FPs given test_true_labels and test_preds\n",
    "test_FPs = []\n",
    "test_FPs_orig_index = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 0 and test_preds[i] == 1:\n",
    "        test_FPs.append(i)\n",
    "        test_FPs_orig_index.append(orig_index[i])\n",
    "\n",
    "# Find test_FNs given test_true_labels and test_preds\n",
    "test_FNs = []\n",
    "test_FNs_orig_index = []\n",
    "for i in range(len(test_true_labels)):\n",
    "    if test_true_labels[i] == 1 and test_preds[i] == 0:\n",
    "        test_FNs.append(i)\n",
    "        test_FNs_orig_index.append(orig_index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_TPs:  1475 test_TNs:  785 test_FPs:  838 test_FNs:  148\n"
     ]
    }
   ],
   "source": [
    "print(\"test_TPs: \", len(test_TPs), \"test_TNs: \", len(test_TNs), \"test_FPs: \", len(test_FPs), \"test_FNs: \", len(test_FNs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(input_ids, attention_mask, true_class, explanations):\n",
    "    # generate an explanation for the input\n",
    "    expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
    "    # normalize scores\n",
    "    expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "    # get the model classification\n",
    "    output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
    "    classification = output.argmax(dim=-1).item()\n",
    "    # get class name\n",
    "    class_name = classifications[classification]\n",
    "    # if the classification is negative, higher explanation scores are more negative\n",
    "    # flip for visualization\n",
    "    if class_name == \"DEAD\":\n",
    "        expl *= (-1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "    tokens_results = [(tokens[i], expl[i].item()) for i in range(len(tokens))]\n",
    "    return tokens_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = Generator(model)\n",
    "\n",
    "def get_many(inputs, masks, labels, explanations):\n",
    "    token_results = []\n",
    "    # viz_data_records = []\n",
    "    \n",
    "    for j in tqdm(range(inputs.shape[0])):\n",
    "        # input_ids, attention_mask, expl, output, classification, tokens, tokens_output = None, None, None, None, None, None, None\n",
    "        input_ids = inputs[j].unsqueeze(0).to(device)\n",
    "        attention_mask = masks[j].unsqueeze(0).to(device)\n",
    "        input_label = labels[j]\n",
    "        # tokens_output, viz = process_one(input_ids, attention_mask, input_label, explanations)\n",
    "        tokens_output = process_one(input_ids, attention_mask, input_label, explanations)\n",
    "        token_results.append(tokens_output)\n",
    "    # viz_data_records.append(viz)\n",
    "\n",
    "    return token_results\n",
    "\n",
    "# import pickle\n",
    "# save_file_path = \"/new-stg/home/banghua/med264/trans_inter/token_results_{}.pkl\".format(i)\n",
    "# with open(save_file_path, 'wb') as f:\n",
    "#     pickle.dump(token_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:12<00:00,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "test_TPs_50_token_results = get_many(test_inputs[test_TPs[:50]], test_masks[test_TPs[:50]], test_labels[test_TPs[:50]], explanations)\n",
    "test_TPs_50_token_results_obj = {\n",
    "    'test_TPs_50_token_results': test_TPs_50_token_results,\n",
    "    'test_TPs_50_orig_index': test_TPs_orig_index[:50]\n",
    "}\n",
    "# Save test_TPs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter_2/test_TPs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_TPs_50_token_results_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:13<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "test_TNs_50_token_results = get_many(test_inputs[test_TNs[:50]], test_masks[test_TNs[:50]], test_labels[test_TNs[:50]], explanations)\n",
    "test_TNs_50_token_results_obj = {\n",
    "    'test_TNs_50_token_results': test_TNs_50_token_results,\n",
    "    'test_TNs_50_orig_index': test_TNs_orig_index[:50]\n",
    "}\n",
    "# Save test_TNs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter_2/test_TNs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_TNs_50_token_results_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:12<00:00,  4.09it/s]\n"
     ]
    }
   ],
   "source": [
    "test_FPs_50_token_results = get_many(test_inputs[test_FPs[:50]], test_masks[test_FPs[:50]], test_labels[test_FPs[:50]], explanations)\n",
    "test_FPs_50_token_results_obj = {\n",
    "    'test_FPs_50_token_results': test_FPs_50_token_results,\n",
    "    'test_FPs_50_orig_index': test_FPs_orig_index[:50]\n",
    "}\n",
    "# Save test_FPs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter_2/test_FPs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_FPs_50_token_results_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:12<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "test_FNs_50_token_results = get_many(test_inputs[test_FNs[:50]], test_masks[test_FNs[:50]], test_labels[test_FNs[:50]], explanations)\n",
    "test_FNs_50_token_results_obj = {\n",
    "    'test_FNs_50_token_results': test_FNs_50_token_results,\n",
    "    'test_FNs_50_orig_index': test_FNs_orig_index[:50]\n",
    "}\n",
    "# Save test_FNs_50_token_results\n",
    "with open(\"/new-stg/home/banghua/med264/trans_inter_2/test_FNs_50_token_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_FNs_50_token_results_obj, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Trans_Inter]",
   "language": "python",
   "name": "conda-env-Trans_Inter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
